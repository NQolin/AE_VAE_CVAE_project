Проект посвящён исследованию и применению трёх типов автоэнкодеров — классического, вариационного и условного вариационного — на примере работы с двумя популярными наборами данных: лицами из LFW (Labeled Faces in the Wild) и цифрами из MNIST.

## 1. Классический Autoencoder (AE) на LFW
В этом разделе используется датасет с изображениями лиц (LFW) и их атрибутами, например, наличие улыбки. Задача автоэнкодера — научиться сжимать изображение в компактное латентное представление и затем восстанавливать его обратно.

Архитектура модели состоит из последовательных свёрточных слоёв в энкодере, затем — линейного слоя, который формирует латентный вектор фиксированной размерности, и декодера с транспонированными свёртками, восстанавливающего изображение.

Помимо реконструкции, на основе латентного пространства можно генерировать новые лица, подавая случайные векторы. Кроме того, в латентном пространстве выделяется вектор, отвечающий за выражение улыбки — благодаря чему можно изменять эмоции на лице, например, превращать «грустное» лицо в улыбающееся.

## 2. Variational Autoencoder (VAE) на MNIST
Применяется вариационный автоэнкодер для работы с изображениями цифр из набора MNIST.

Энкодер последовательно применяет свёртки, пулинг и преобразование в плоский вектор, после чего два линейных слоя вычисляют параметры распределения — среднее (μ) и логарифм дисперсии (logσ). Далее с помощью трюка повторного параметрирования происходит сэмплирование латентного вектора z, который затем декодируется обратно в изображение.

Обучение VAE проходит с учётом двух компонентов функции потерь: качества реконструкции (Binary Cross Entropy) и регуляризации латентного пространства (KL-дивергенция), которая заставляет распределение латентных векторов приближаться к стандартному нормальному.

После обучения модель способна генерировать новые цифры из случайных латентных векторов, а также визуализировать структуру латентного пространства с помощью метода t-SNE, показывая, как цифры разных классов группируются по отдельным кластерам.

## 3. Conditional Variational Autoencoder (CVAE) на MNIST
CVAE — это расширение VAE, в котором дополнительно учитывается класс цифры при генерации. Это позволяет не просто создавать произвольные цифры, а задавать конкретный класс для генерации.
В архитектуре энкодер также выдаёт параметры распределения латентного вектора, а декодер принимает на вход по-мимо сэмплированного вектора z вектор меток в формате one-hot, указывающий желаемый класс.

Обучение происходит аналогично VAE, с той же функцией потерь. Это позволяет условно генерировать цифры заданного класса и исследовать, насколько метки помогают лучше структурировать латентное пространство.

## Установка
pip install torch torchvision matplotlib scikit-learn numpy pandas scikit-image


